<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="IE=edge" http-equiv="X-UA-Compatible"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <script src="https://cdn.tailwindcss.com" type="text/javascript">
  </script>
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1.28.0/themes/prism.min.css" rel="stylesheet"/>
  <link href="https://raw.githubusercontent.com/PrismJS/prism-themes/master/themes/prism-atom-dark.css" rel="stylesheet"/>
  <title>
   Default title
  </title>
 </head>
 <body>
  <div class="font-serif">
   <div class="grid grid-cols-1 mx-3 lg:grid-cols-10 w-100 lg:h-screen" id="navbar">
    <div class="h-100 overflow-hidden justify-center flex lg:flex-col flex-row text-center col-span-1">
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/" id="Home">
      Home
     </a>
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/posts.html" id="Posts">
      Posts
     </a>
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/intro.html" id="About">
      About
     </a>
     <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="/sitemap.xml" id="Sitemap">
      Sitemap
     </a>
    </div>
    <div class="lg:border-r-2 lg:border-l-2 col-span-7 lg:overflow-y-scroll sm:flex sm:flex-wrap" id="content">
     <div class="flex flex-wrap">
      <div class="border-b-2 my-2 p-3">
       <h1 class="text-4xl font-bold">
        "Offline Speech Recognition with Vosk"
       </h1>
       <div class="py-3 my-3 text-2xl font-semibold text-gray-500" id="summary">
       </div>
       <div class="px-2 py-5 text-gray-700">
        <p id="published_on">
         Date Published: 2021-07-24T20:43:12+05:30
        </p>
       </div>
      </div>
      <div class="px-3 py-2 my-2 mx-3 space-y-3 text-xl">
       <h2 id="no-more-sphinx">
        No more Sphinx
       </h2>
       <p>
        The long-lived and long-loved CMU Sphinx, a brainchild of Carnegie Mellon University, is not maintained actively anymore, since 5 years. But does that mean that we need to move to more production-oriented solutions? No, we actually don't. The team CMU Sphinx Project has slowly rolled in a new child project -
        <a href="https://alphacephei.com/vosk">
         <strong>
          Vosk
         </strong>
        </a>
        .
       </p>
       <p>
        Note that there are many other production-oriented solutions available (like OpenVINO, Mozilla DeepSpeech, etc.), which are equally as good, if not better at speech recognition. I am focusing on the ease of setup and use. üòÖ
       </p>
       <h2 id="okay-i-dont-know-what-you-are-talking-about-please-explain-more">
        Okay, I don't know what you are talking about. Please explain more.
       </h2>
       <p>
        Quoting the
        <a href="https://cmusphinx.github.io/wiki/about/">
         Official CMU Sphinx wiki's About section
        </a>
        (forgive me for being lazy):
       </p>
       <p>
        <img alt="CMUSphinx Wiki About Section" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626621691957/5JfVUHJxE.png"/>
       </p>
       <h3 id="i-get-it-but-why-do-you-call-this-dead">
        I get it, but why do you call this dead?
       </h3>
       <p>
        This is the screenshot of the two most recent posts on the
        <a href="https://cmusphinx.github.io/">
         CMU Sphinx Official Blog
        </a>
        :
       </p>
       <p>
        <img alt="CMUSphinx blog" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626466904859/9OZ2VX4IU.png"/>
       </p>
       <p>
        Also this
        <a href="https://news.ycombinator.com/item?id=13040523">
         discussion
        </a>
        from YCombinator:
       </p>
       <p>
        <img alt="YCombinator thread" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626467333151/gAridT3-q.png"/>
       </p>
       <p>
        Even if I disagree with the YCombinator discussion, the official CMU Sphinx blog does little to give me confidence.
       </p>
       <h2 id="okay-i-get-it-so-what-now">
        Okay I get it. So what now?
       </h2>
       <p>
        Another screenshot from the main
        <a href="https://cmusphinx.github.io/">
         CMU Sphinx website
        </a>
        :
       </p>
       <p>
        <img alt="CMUSphinx page head" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626467505926/MDDCitxMU.png"/>
       </p>
       <p>
        Not gonna lie, I was pretty disappointed üôÅ. I've been a Sphinx user for quite sometime. I'm no researcher, but I was actually familiar with Sphinx. So I wondered how Vosk would do for me. And I was really surprised at the gentle learning curve to implement Vosk to my apps. But there is really less documentation at the time of writing this blog. I hope this post will fill up some of that gap.
       </p>
       <p>
        Anyways, enough chatter. So in this post, I am going to show you how to setup a simple Python script to recognize your speech, using it alongside NLTK to identify your speech and extract the keywords. The end result? A fully functional system that takes your voice input and processes it reasonably accurately, so that you can add voice control features to any awesome projects you may be building! üòÉ
       </p>
       <h2 id="setting-up">
        Setting up:
       </h2>
       <h3 id="stage-0-resolving-system-level-dependencies">
        Stage 0: Resolving system-level dependencies:
       </h3>
       <blockquote>
        <p>
         "Know thy tools." ~ Some great person.
        </p>
       </blockquote>
       <p>
        Okay so before I start, let's see with what we'll be working on:
       </p>
       <ol>
        <li>
         A Linux System (Ubuntu in my case). Windows and Mac users, don't be disheartened - the programming part is the same for all.
        </li>
        <li>
         PulseAudio Audio Drivers
        </li>
        <li>
         Python 3.8 with pip working.
        </li>
        <li>
         A working Internet connection
        </li>
        <li>
         An IDE (preferably) (VSCode in my case)
        </li>
        <li>
         A microphone (or a headphone or earphone with an attached microphone)
        </li>
       </ol>
       <p>
        So first, we need to install the appropriate
        <code>
         pulseaudio
        </code>
        ,
        <code>
         alsa
        </code>
        and
        <code>
         jack
        </code>
        drivers, among others.
       </p>
       <p>
        Assuming you're running Debian (or Ubuntu), type the following commands:
       </p>
       <pre><code>sudo apt-get install gcc automake autoconf libtool bison swig python3-dev
sudo apt-get install libpulse-dev jackd libasound2-dev
</code></pre>
       <p>
        <strong>
         Note
        </strong>
        : Don't try to combine the above 2 statements (no pro-gamer move now üòú).
        <code>
         libasound2-dev
        </code>
        and
        <code>
         jackd
        </code>
        require
        <code>
         swig
        </code>
        to build their driver codes.
       </p>
       <p>
        If you're familiar with CMU Sphinx, you'd realise that there are a lot of common dependencies - which is no coincidence. Vosk comes from Sphinx itself.
       </p>
       <p>
        <em>
         If you face some issues with installing
         <code>
          swig
         </code>
         , don't worry. Just Google your error with the keyword CMU Sphinx.
        </em>
       </p>
       <h3 id="stage-1-setting-up-vosk-api">
        Stage 1: Setting up Vosk-API
       </h3>
       <p>
        First, we need to download Vosk-API. The Vosk API needs less setup, compared to the original source code.
       </p>
       <p>
        Assuming you have git installed on your system, enter in your terminal:
       </p>
       <pre><code>git clone https://github.com/alphacep/vosk-api.git
</code></pre>
       <p>
        If you don't have git, or have some other issues with it,
        <a href="https://github.com/alphacep/vosk-api">
         download Vosk-API from here
        </a>
        .
       </p>
       <ol>
        <li>
         <p>
          Create a project folder (say speech2command). Download (or clone) the Vosk-api code into a subfolder there.
         </p>
        </li>
        <li>
         <p>
          Now extract the .zip file (or .tar.gz file) into your project folder (if you downloaded the source code as an archive).
         </p>
        </li>
       </ol>
       <p>
        Your directory structure should look something like this:
       </p>
       <pre><code>speech2command
     |_______ vosk-api
     |_______ ...
</code></pre>
       <p>
        Now we're good to go.
       </p>
       <h3 id="stage-2-setting-up-a-language-model">
        Stage 2: Setting up a language model
       </h3>
       <p>
        The versatility of Vosk (or CMUSphinx) comes from its ability to use models to recognize various languages.
       </p>
       <p>
        Simply put, models are the parts of Vosk that are language-specific and supports speech in different languages. At the time of writing, Vosk has support for more than 18 languages including Greek, Turkish, Chinese, Indian English, etc.
       </p>
       <p>
        <a href="https://alphacephei.com/vosk/models">
         The Vosk Model Wiki
        </a>
       </p>
       <p>
        In this post, we are going to use the
        <a href="http://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip">
         small American English model
        </a>
        . It's compact (around 40 Mb) and reasonably accurate.
       </p>
       <p>
        Download the model and extract it in your project folder. Rename the folder you extracted from the .zip file as
        <code>
         model
        </code>
        . Now, your directory structure should look like this:
       </p>
       <pre><code>speech2command
     |_______ vosk-api
     |_______ model
     |_______ ...
</code></pre>
       <p>
        Here is a video walkthrough (albeit a bit old):
       </p>
       <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="315" src="https://www.youtube.com/embed/Itic1lFc4Gg" title="YouTube video player" width="560">
       </iframe>
       <p>
        <br/>
       </p>
       <h3 id="stage-3-setting-up-python-packages">
        Stage 3: Setting up Python Packages
       </h3>
       <p>
        For our project, we need the following Python packages:
       </p>
       <ol>
        <li>
         platform
        </li>
        <li>
         Speech Recognition
        </li>
        <li>
         NLTK
        </li>
        <li>
         JSON
        </li>
        <li>
         sys
        </li>
        <li>
         Vosk
        </li>
       </ol>
       <p>
        The packages
        <code>
         platform
        </code>
        ,
        <code>
         sys
        </code>
        and
        <code>
         json
        </code>
        come included in a standard Python 3 installation. We need to install the other packages manually.
       </p>
       <p>
        In the command line, type:
       </p>
       <pre><code>pip install nltk speech_recognition vosk
</code></pre>
       <p>
        Wait as the components get installed one by one.
       </p>
       <h3 id="stage-4-setting-up-nltk-packages">
        Stage 4: Setting up NLTK Packages
       </h3>
       <p>
        Now NLTK is a huge package, with a dedicated index to manage its components. We just downloaded the NLTK core components to get a basic program up and running. We need a few more NLTK components to add to continue with the code.
       </p>
       <p>
        The required packages are:
        <code>
         stopwords
        </code>
        ,
        <code>
         averaged_perceptron_tagger
        </code>
        ,
        <code>
         punkt
        </code>
        , and
        <code>
         wordnet
        </code>
        .
       </p>
       <pre><code>nltk.download("stopwords")
nltk.download("averaged_perceptron_tagger")
nltk.download("punkt")
nltk.download("wordnet")
</code></pre>
       <p>
        Or in one line:
       </p>
       <pre><code>nltk.download("stopwords", "averaged_perceptron_tagger", "punkt", "wordnet")
</code></pre>
       <h3 id="stage-5-programming-with-vosk-and-nltk">
        Stage 5: Programming with Vosk and NLTK.
       </h3>
       <p>
        Here comes the fun part! Let's code something in Python to identify speech and convert it to text, using Vosk-API as the backend.
       </p>
       <p>
        Make a new Python file (say
        <code>
         s2c.py
        </code>
        ) in your project folder. Now the project folder directory structure should look like:
       </p>
       <pre><code>speech2command
     |_______ vosk-api
     |_______ model
     |_______ s2c.py
     |_______ ...
</code></pre>
       <h4 id="coding-time-now">
        Coding time now! ü§©
       </h4>
       <p>
        Okay, so the code for the project is given below. The code is pretty clean (or so I hope), and you can understand the code yourself (or just copy-paste it üòú).
       </p>
       <pre><code>import platform
import speech_recognition as sr
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords, wordnet
import sys
import vosk
import json
from vosk import SetLogLevel

SetLogLevel(-1) # Hide Vosk logs

p = platform.system()


def listen():
    rec = sr.Recognizer()
    with sr.Microphone() as src:
        rec.adjust_for_ambient_noise(src)
        audio = rec.listen(src)
    try:
        cmd = rec.recognize_vosk(audio) # Connecting to Vosk API
    except Exception:
        print("Sorry, couldn't hear. Mind trying typing it?")
        cmd = input()
    return cmd


def pos_tagger(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return None


def lemmatizer(src):
    w = WordNetLemmatizer()
    pos_tagged = nltk.pos_tag(nltk.word_tokenize(src))
    wn_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))
    ls = []  # lemmatized sentence
    for word, tag in wn_tagged:
        if tag is None:
            ls.append(word)
        else:
            ls.append(w.lemmatize(word, tag))
    return ls


def make_tokens(lms):
    stop_words = set(stopwords.words('english')
    src3 = []
    for i in lms:
        if i in stop_words:
            pass
        else:
            src3.append(str(i)+" ")
    print("Keywords are:", end=' ')
    for i in src3:
        print(i, end=' ')


try:
    while True:
        print("\nSay some words: ")
        c = listen()
        print("Listened value=",c)
        d = json.loads(c)
        print("Command =", d["text"])
        if str(d["text"]).rstrip(" ") in ['stop', 'exit', 'bye', 'quit', 'terminate', 'kill', 'end']:
            print("\n\nExit command triggered from command! Exiting...")
            sys.exit()
        lemmatized = lemmatizer(d["text"])
        make_tokens(lemmatized)
except KeyboardInterrupt:
    print("\n\nExit command triggered from Keyboard! Exiting...")
</code></pre>
       <p>
        Now run this code, and this will set up a listener that works continuously - with some verbose logs as well - which you can see on your terminal screen. Ignore those logs, they are just for information.
       </p>
       <h4 id="if-you-need-the-source-code-i-made-a-repo-for-it-vosk-demo">
        If you need the source code, I made a repo for it:
        <a href="https://github.com/anuran-roy/vosk-demo">
         Vosk Demo
        </a>
       </h4>
       <h3 id="explanation">
        Explanation:
       </h3>
       <p>
        Here is a flowchart that shows exactly how this works:
       </p>
       <p>
        <img alt="Project mechanism flowchart" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1626590492431/US0gqw6pN.png"/>
       </p>
       <p>
        So this was it, folks! Enjoy your very own speech2text (or rather, speech2command) recognition system. Keep tinkering!
       </p>
       <p>
        Bye for now!
        <br/>
        <strong>
         <em>
          Ron
         </em>
        </strong>
       </p>
      </div>
     </div>
     <div class="grid lg:grid-cols-8 grid-cols-1 w-100 my-5 py-5" id="navigation">
      <a class="justify-center hover:bg-gray-800 rounded-md hover:text-white font-bold border-2 col-span-1 lg:col-span-2 text-center text-gray-700 px-5 py-3 mx-5" href="https://anuran-roy.github.io/writings/blog/machine-learning.html">
       ‚¨Ö¬†¬†Previous Post
      </a>
      <div class="col-span-1 md:cols-span-1 lg:col-span-4 px-5 mx-5 py-3">
      </div>
      <a class="justify-center hover:bg-gray-800 rounded-md hover:text-white border-2 font-bold col-span-1 lg:col-span-2 text-center text-gray-700 px-5 py-3 mx-5" href="/blog/why-not-global-settings-in-projects.html">
       Next Post¬†¬†‚û°
      </a>
     </div>
     <div class="invisible py-5 my-5">
     </div>
    </div>
    <div class="lg:top-0 lg:bottom-0 lg:h-screen justify-center content-center align-center object-center flex flex-col lg:col-span-2 text-center m-3" id="meta">
     <p class="flex justify-center items-center flex-col">
      <img alt="Author Picture" class="rounded-full py-5 my-5" src="https://pbs.twimg.com/profile_images/1421779872364564482/vZljyGHa_400x400.jpg" width="30%"/>
      <div class="text-2xl text-center font-bold">
       About
      </div>
      Lorem ipsum dolor sit, amet consectetur adipisicing elit. Accusantium, accusamus explicabo a veritatis culpa, expedita quos illo quibusdam earum mollitia molestiae ducimus sed sunt sit. Magnam earum quia est a iure voluptatem eligendi excepturi place
     </p>
     <p class="flex flex-col lg:columns-3 columns-1 py-5 my-5">
      <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="http://github.com/anuran-roy/">
       GitHub
      </a>
      <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="http://www.linkedin.com/in/anuran-roy/">
       LinkedIn
      </a>
      <a class="m-2 py-2 hover:bg-gray-800 hover:text-white rounded-md" href="http://twitter.com/AnuranRoy">
       Twitter
      </a>
     </p>
    </div>
   </div>
   <div class="fixed left-0 right-0 bottom-0 bg-gray-100 opacity-80 hover:opacity-100 p-3 align-center flex justify-center items-center flex-col" id="footer">
    <span class="text-gray-700 text-center">
     Using Blogtastic theme in
     <a class="font-bold text-blue-500" href="https://github.com/fossworx-labs/fossfolio">
      FOSSfolio
     </a>
     . Made by
     <a class="font-bold text-blue-500" href="https://linktr.ee/anuranroy">
      Anuran
     </a>
     with üíô
    </span>
   </div>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.28.0/prism.min.js">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.28.0/plugins/autoloader/prism-autoloader.min.js">
  </script>
 </body>
</html>